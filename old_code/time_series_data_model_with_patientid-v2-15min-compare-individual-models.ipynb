{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"/data2/yinterian/multi-task-romain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15min'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap = \"15min\"\n",
    "gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data_train_{gap}.pickle\".format(gap=gap)\n",
    "with open(PATH/filename, 'rb') as f:\n",
    "    train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data_valid_{gap}.pickle\".format(gap=gap)\n",
    "with open(PATH/filename, 'rb') as f:\n",
    "    valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42830, 14), (5069, 14))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id_list = np.sort(np.unique(train.subject_id.values))\n",
    "id2index = {v: k+1 for k,v in enumerate(subject_id_list)}\n",
    "num_subjects = len(subject_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2170"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std_series(train):\n",
    "    ss = np.concatenate(train.series.values)\n",
    "    ss = ss.reshape(-1,5)\n",
    "    return ss.mean(axis=0), ss.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std_static(train):\n",
    "    res = {}\n",
    "    for name in [\"age\", \"sapsii\", \"sofa\"]:\n",
    "        values = train[name].values\n",
    "        res[name] = (values.mean(), values.std())\n",
    "    res[\"series\"] = get_mean_std_series(train)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': (64, 15.087455295966063),\n",
       " 'sapsii': (33, 14.265492481117855),\n",
       " 'sofa': (4, 3.7831641172054082),\n",
       " 'series': (array([ 83.19453341,  93.64397046, 121.07613603,  58.73969887,\n",
       "          78.6694367 ]),\n",
       "  array([16.08727268, 17.53684697, 21.3399693 , 12.26982071, 14.36323955]))}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_dict = get_mean_std_static(train)\n",
    "norm_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, 2: 1, 3: 2, 5: 3, 6: 4}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "care2id = {v:k for k,v in enumerate(np.unique(train.care_unit.values))}\n",
    "care2id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTask(Dataset):\n",
    "    def __init__(self, df, norm_dict, id2index, care2id, k=20, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: dataframe with data\n",
    "            norm_dict: mean and std of all variables to normalize\n",
    "            \n",
    "        \"\"\"\n",
    "        self.norm_dict = norm_dict\n",
    "        self.df = df\n",
    "        self.df[\"care_unit\"] = self.df[\"care_unit\"].apply(lambda x: care2id[x])\n",
    "        self.names = [\"age\", \"sapsii\", \"sofa\"] ## needs normalization\n",
    "        self.names_binary = [\"gender\", \"amine\", \"sedation\", \"ventilation\"]\n",
    "        self.id2index = id2index\n",
    "        self.train = train\n",
    "        self.df_sample = self.pick_a_sample(k)\n",
    "            \n",
    "    def pick_a_sample(self, k=20):\n",
    "        \"\"\" Picks sample with the same number of observations per patient\"\"\"\n",
    "        #if not self.train: # fix seed for validation and test\n",
    "        #    np.random.seed(3)\n",
    "        sample = self.df.groupby(\"subject_id\", group_keys=False).apply(lambda x: x.sample(min(len(x), k)))\n",
    "        sample = sample.copy()\n",
    "        if self.train:\n",
    "            self.subject_index = [self.id2index[subject_id] for subject_id in sample.subject_id.values]\n",
    "            self.random = np.random.choice(2, sample.shape[0], p=[0.1, 0.9])\n",
    "            self.subject_index = self.subject_index*self.random\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df_sample.iloc[index,:]\n",
    "        x_series = (row.series - self.norm_dict[\"series\"][0])/self.norm_dict[\"series\"][1]\n",
    "        x_cont = [(row[name]-self.norm_dict[name][0])/self.norm_dict[name][1] for name in self.names]\n",
    "        x_binary = [row[name] for name in self.names_binary]\n",
    "        subject_index = 0\n",
    "        if self.train:\n",
    "            subject_index = self.subject_index[index]\n",
    "        x_cat = np.array([row[\"care_unit\"], subject_index])\n",
    "        x_cont = np.array(x_cont + x_binary)\n",
    "        return x_series, x_cont, x_cat, row[\"prediction_mean_HR\"], row[\"prediction_mean_MAP\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MultiTask(train, norm_dict, id2index, care2id)\n",
    "valid_ds = MultiTask(valid, norm_dict, id2index, care2id, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, valid_dl, which_y=\"y1\"):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    y_hat = []\n",
    "    ys = []\n",
    "    for x_series, x_cont, x_cat, y1, y2 in valid_dl:\n",
    "        batch = y1.shape[0]\n",
    "        x_series = x_series.float().cuda()\n",
    "        x_cont = x_cont.float().cuda()\n",
    "        x_cat = x_cat.long().cuda()\n",
    "        y1 = y1.float().cuda()\n",
    "        y2 = y2.float().cuda()\n",
    "        out = model(x_series, x_cont, x_cat)\n",
    "        if which_y==\"y1\":\n",
    "            mse_loss = F.mse_loss(out, y1.unsqueeze(-1))\n",
    "            ys.append(y1.view(-1).cpu().numpy())\n",
    "        else:\n",
    "            mse_loss = F.mse_loss(out, y2.unsqueeze(-1))\n",
    "            ys.append(y2.view(-1).cpu().numpy())\n",
    "        sum_loss += batch*(mse_loss.item())\n",
    "        total += batch\n",
    "        y_hat.append(out.view(-1).detach().cpu().numpy())\n",
    "    \n",
    "    y_hat = np.concatenate(y_hat)\n",
    "    ys = np.concatenate(ys)\n",
    "    r2 = metrics.r2_score(ys, y_hat)\n",
    "    \n",
    "    return sum_loss/total, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, train_ds, optimizer, lr=1e-3, epochs = 30, which_y=\"y1\"):\n",
    "    prev_val_r2 = 0\n",
    "    for i in range(epochs):\n",
    "        sum_loss = 0\n",
    "        total = 0\n",
    "        train_ds.pick_a_sample()\n",
    "        train_dl = DataLoader(train_ds, batch_size=5000, shuffle=True)\n",
    "        for x_series, x_cont, x_cat, y1, y2 in train_dl:\n",
    "            model.train()\n",
    "            x_series = x_series.float().cuda()\n",
    "            x_cont = x_cont.float().cuda()\n",
    "            x_cat = x_cat.long().cuda()\n",
    "            y1 = y1.float().cuda()\n",
    "            y2 = y2.float().cuda()\n",
    "            out = model(x_series, x_cont, x_cat)\n",
    "            if which_y==\"y1\":\n",
    "                loss = F.mse_loss(out, y1.unsqueeze(-1))\n",
    "            else:\n",
    "                loss = F.mse_loss(out, y2.unsqueeze(-1))\n",
    "            sum_loss += len(y1) * loss.item()\n",
    "            \n",
    "            total += len(y1)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if i % 1 == 0:\n",
    "            val_loss, val_r2= val_metrics(model, valid_dl, which_y=which_y)\n",
    "            print(\"\\tTrain loss: {:.3f} valid loss: {:.3f} valid r2 {:.3f}\".format(\n",
    "                sum_loss/total, val_loss, val_r2))\n",
    "        if val_r2 > prev_val_r2:\n",
    "            prev_val_r2 = val_r2\n",
    "            if val_r2 > 0.7:\n",
    "                filename = \"single_model_\" + which_y\n",
    "                path = \"{0}/models/{1}_r2_{2:.0f}.pth\".format(PATH, filename, 100*val_r2) \n",
    "                save_model(model, path)\n",
    "                print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventModel3(nn.Module):\n",
    "    def __init__(self, hidden_size=100, num2=50, num_units=5):\n",
    "        super(EventModel3, self).__init__()\n",
    "        self.embedding1 = nn.Embedding(num_units, 1)\n",
    "        self.embedding2 = nn.Embedding(num_subjects+1, 5)\n",
    "        self.gru = nn.GRU(5, hidden_size, batch_first=True)\n",
    "        self.num1 = hidden_size + 1 + 5 + 7\n",
    "        self.num2 = num2\n",
    "        self.linear1 = nn.Linear(self.num1, self.num2)\n",
    "        self.linear2 = nn.Linear(self.num2, self.num2)\n",
    "        self.out = nn.Linear(self.num2, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(self.num2)\n",
    "        self.bn2 = nn.BatchNorm1d(self.num2)\n",
    "        \n",
    "    def forward(self, x_series, x_cont, x_cat):\n",
    "        _, ht = self.gru(x_series)\n",
    "        x_cat_1 = self.embedding1(x_cat[:,0])\n",
    "        x_cat_2 = self.embedding2(x_cat[:,1])\n",
    "        x = torch.cat((ht[-1], x_cat_1, x_cat_2, x_cont), 1)\n",
    "        x = self.bn1(F.relu(self.linear1(x)))\n",
    "        x = self.bn2(F.relu(self.linear2(x)))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EventModel3().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_series, x_cont, x_cat, y_HR, y_MAP = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_series = x_series.float().cuda()\n",
    "x_cont = x_cont.float().cuda()\n",
    "x_cat = x_cat.long().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x_series, x_cont, x_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss: 7042.043 valid loss: 6933.797 valid r2 -26.036\n",
      "\tTrain loss: 6563.981 valid loss: 4335.460 valid r2 -15.905\n",
      "\tTrain loss: 5863.749 valid loss: 4535.086 valid r2 -16.683\n",
      "\tTrain loss: 4907.551 valid loss: 4177.601 valid r2 -15.289\n",
      "\tTrain loss: 3663.166 valid loss: 2694.306 valid r2 -9.506\n",
      "\tTrain loss: 2174.319 valid loss: 1101.661 valid r2 -3.296\n",
      "\tTrain loss: 861.754 valid loss: 204.086 valid r2 0.204\n",
      "\tTrain loss: 146.929 valid loss: 28.701 valid r2 0.888\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y1_r2_89.pth\n",
      "\tTrain loss: 56.943 valid loss: 253.329 valid r2 0.012\n",
      "\tTrain loss: 199.002 valid loss: 274.183 valid r2 -0.069\n",
      "\tTrain loss: 166.185 valid loss: 101.982 valid r2 0.602\n",
      "\tTrain loss: 51.219 valid loss: 17.630 valid r2 0.931\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y1_r2_93.pth\n",
      "\tTrain loss: 13.338 valid loss: 16.924 valid r2 0.934\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y1_r2_93.pth\n",
      "\tTrain loss: 27.988 valid loss: 32.647 valid r2 0.873\n",
      "\tTrain loss: 31.105 valid loss: 27.496 valid r2 0.893\n"
     ]
    }
   ],
   "source": [
    "# model for mean_HR\n",
    "model = EventModel3().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03, weight_decay=1e-5)\n",
    "train_epochs(model, train_ds, optimizer, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss: 21.012 valid loss: 45.213 valid r2 0.824\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y1_r2_82.pth\n",
      "\tTrain loss: 14.094 valid loss: 38.205 valid r2 0.851\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y1_r2_85.pth\n",
      "\tTrain loss: 12.965 valid loss: 19.784 valid r2 0.923\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y1_r2_92.pth\n",
      "\tTrain loss: 11.700 valid loss: 15.851 valid r2 0.938\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y1_r2_94.pth\n",
      "\tTrain loss: 10.837 valid loss: 14.753 valid r2 0.942\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y1_r2_94.pth\n",
      "\tTrain loss: 10.553 valid loss: 12.614 valid r2 0.951\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y1_r2_95.pth\n",
      "\tTrain loss: 10.739 valid loss: 10.908 valid r2 0.957\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y1_r2_96.pth\n",
      "\tTrain loss: 10.512 valid loss: 12.008 valid r2 0.953\n",
      "\tTrain loss: 10.671 valid loss: 15.540 valid r2 0.939\n",
      "\tTrain loss: 10.669 valid loss: 12.326 valid r2 0.952\n",
      "\tTrain loss: 10.262 valid loss: 11.111 valid r2 0.957\n",
      "\tTrain loss: 9.873 valid loss: 11.982 valid r2 0.953\n",
      "\tTrain loss: 9.649 valid loss: 12.226 valid r2 0.952\n",
      "\tTrain loss: 9.443 valid loss: 12.127 valid r2 0.953\n",
      "\tTrain loss: 9.306 valid loss: 12.855 valid r2 0.950\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.02, weight_decay=1e-5)\n",
    "train_epochs(model, train_ds, optimizer, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss: 6202.056 valid loss: 7171.359 valid r2 -38.460\n",
      "\tTrain loss: 5802.007 valid loss: 6970.132 valid r2 -37.352\n",
      "\tTrain loss: 5181.522 valid loss: 6550.658 valid r2 -35.044\n",
      "\tTrain loss: 4304.595 valid loss: 4648.040 valid r2 -24.575\n",
      "\tTrain loss: 3150.987 valid loss: 2800.306 valid r2 -14.408\n",
      "\tTrain loss: 1827.713 valid loss: 937.386 valid r2 -4.158\n",
      "\tTrain loss: 658.341 valid loss: 173.789 valid r2 0.044\n",
      "\tTrain loss: 93.757 valid loss: 33.124 valid r2 0.818\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y2_r2_82.pth\n",
      "\tTrain loss: 88.739 valid loss: 109.755 valid r2 0.396\n",
      "\tTrain loss: 205.611 valid loss: 132.688 valid r2 0.270\n",
      "\tTrain loss: 136.892 valid loss: 44.479 valid r2 0.755\n",
      "\tTrain loss: 38.684 valid loss: 18.951 valid r2 0.896\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y2_r2_90.pth\n",
      "\tTrain loss: 23.894 valid loss: 35.686 valid r2 0.804\n",
      "\tTrain loss: 38.813 valid loss: 36.549 valid r2 0.799\n",
      "\tTrain loss: 35.036 valid loss: 25.053 valid r2 0.862\n"
     ]
    }
   ],
   "source": [
    "# mean_MAP\n",
    "model = EventModel3().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03, weight_decay=1e-5)\n",
    "train_epochs(model, train_ds, optimizer, epochs=15, which_y=\"y2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss: 24.718 valid loss: 32.093 valid r2 0.823\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y2_r2_82.pth\n",
      "\tTrain loss: 20.594 valid loss: 27.351 valid r2 0.850\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y2_r2_85.pth\n",
      "\tTrain loss: 19.094 valid loss: 30.055 valid r2 0.835\n",
      "\tTrain loss: 18.521 valid loss: 18.130 valid r2 0.900\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y2_r2_90.pth\n",
      "\tTrain loss: 18.064 valid loss: 18.547 valid r2 0.898\n",
      "\tTrain loss: 17.512 valid loss: 16.253 valid r2 0.911\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y2_r2_91.pth\n",
      "\tTrain loss: 17.135 valid loss: 16.223 valid r2 0.911\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y2_r2_91.pth\n",
      "\tTrain loss: 16.969 valid loss: 16.010 valid r2 0.912\n",
      "/data2/yinterian/multi-task-romain/models/single_model_y2_r2_91.pth\n",
      "\tTrain loss: 16.671 valid loss: 16.593 valid r2 0.909\n",
      "\tTrain loss: 16.403 valid loss: 16.338 valid r2 0.910\n",
      "\tTrain loss: 16.324 valid loss: 17.176 valid r2 0.905\n",
      "\tTrain loss: 16.449 valid loss: 16.730 valid r2 0.908\n",
      "\tTrain loss: 15.925 valid loss: 17.979 valid r2 0.901\n",
      "\tTrain loss: 15.992 valid loss: 18.343 valid r2 0.899\n",
      "\tTrain loss: 15.766 valid loss: 17.177 valid r2 0.905\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.02, weight_decay=1e-5)\n",
    "train_epochs(model, train_ds, optimizer, epochs=15, which_y=\"y2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = PATH/\"models/single_model_y1_r2_96.pth\"\n",
    "model = EventModel3().cuda()\n",
    "load_model(model, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename = \"data_test_{gap}.pickle\".format(gap=gap)\n",
    "with open(PATH/filename, 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5933, 14), 3794)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = MultiTask(test, norm_dict, id2index, care2id, k=25, train=False)\n",
    "test.shape, len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_ds, batch_size=4561)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.908259391784668, 0.9574664495909214)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(model, valid_dl, which_y=\"y1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = PATH/\"models/single_model_y2_r2_91.pth\"\n",
    "load_model(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16.00960922241211, 0.9119090799797106)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(model, valid_dl, which_y=\"y2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9335"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.957 + 0.91)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r2_with_ci(model, df, which_y, n=1000):\n",
    "    losses = []\n",
    "    r2s = []\n",
    "    for i in range(n):\n",
    "        test_ext_ds = MultiTask(df.copy(), norm_dict, id2index, care2id, k=13, train=False)\n",
    "        test_ext_dl = DataLoader(test_ext_ds, batch_size=len(test_ext_ds))\n",
    "        loss, r2 = val_metrics(model, test_ext_dl, which_y=which_y)\n",
    "        losses.append(loss)\n",
    "        r2s.append(r2)\n",
    "    return losses, r2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data_validation_15min.pickle\"\n",
    "with open(PATH/filename, 'rb') as f:\n",
    "    test_ext = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = PATH/\"models/single_model_y1_r2_96.pth\"\n",
    "model = EventModel3().cuda()\n",
    "load_model(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, r2s = get_r2_with_ci(model, test_ext, which_y=\"y1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = PATH/\"models/single_model_y2_r2_91.pth\"\n",
    "load_model(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_y2, r2s_y2 = get_r2_with_ci(model, test_ext, which_y=\"y2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82874327, 0.85672872, 0.88146229])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(r2s, [0.025, 0.5, 0.975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73040045, 0.76939074, 0.80461116])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(r2s_y2, [0.025, 0.5, 0.975])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
