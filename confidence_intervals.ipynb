{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"/data2/yinterian/multi-task-romain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing stats and maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_gap(PATH, gap=\"60min\"):\n",
    "    filename = \"data_train_{gap}.pickle\".format(gap=gap)\n",
    "    with open(PATH/filename, 'rb') as f:\n",
    "        train = pickle.load(f)\n",
    "    filename = \"data_valid_{gap}.pickle\".format(gap=gap)\n",
    "    with open(PATH/filename, 'rb') as f:\n",
    "        valid = pickle.load(f)\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_gap(PATH, gap=\"60min\"):\n",
    "    filename1 = \"data_validation_{gap}.pickle\".format(gap=gap)\n",
    "    with open(PATH/filename1, 'rb') as f:\n",
    "        test_ext = pickle.load(f)\n",
    "    \n",
    "    filename2 = \"data_test_{gap}.pickle\".format(gap=gap)\n",
    "    with open(PATH/filename2, 'rb') as f:\n",
    "        test = pickle.load(f)\n",
    "    print(filename1, filename2)\n",
    "    return test_ext, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std_series(train):\n",
    "    ss = np.concatenate(train.series.values)\n",
    "    ss = ss.reshape(-1,5)\n",
    "    return ss.mean(axis=0), ss.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std_static(train):\n",
    "    res = {}\n",
    "    for name in [\"age\", \"sapsii\", \"sofa\"]:\n",
    "        values = train[name].values\n",
    "        res[name] = (values.mean(), values.std())\n",
    "    res[\"series\"] = get_mean_std_series(train)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_dict(train):\n",
    "    subject_id_list = np.sort(np.unique(train.subject_id.values))\n",
    "    id2index = {v: k+1 for k,v in enumerate(subject_id_list)}\n",
    "    num_subjects = len(subject_id_list)\n",
    "    norm_dict = get_mean_std_static(train)\n",
    "    care2id = {v:k for k,v in enumerate(np.unique(train.care_unit.values))}\n",
    "    return norm_dict, care2id, id2index, num_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTask(Dataset):\n",
    "    def __init__(self, df, norm_dict, id2index, care2id,  k=20, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: dataframe with data\n",
    "            norm_dict: mean and std of all variables to normalize\n",
    "            \n",
    "        \"\"\"\n",
    "        self.norm_dict = norm_dict\n",
    "        self.df = df\n",
    "        self.df[\"care_unit\"] = self.df[\"care_unit\"].apply(lambda x: care2id[x])\n",
    "        self.names = [\"age\", \"sapsii\", \"sofa\"] ## needs normalization\n",
    "        self.names_binary = [\"gender\", \"amine\", \"sedation\", \"ventilation\"]\n",
    "        self.id2index = id2index\n",
    "        self.train = train\n",
    "        self.pick_a_sample(k)\n",
    "            \n",
    "    def pick_a_sample(self, k=20):\n",
    "        \"\"\" Picks sample with the same number of observations per patient\"\"\"\n",
    "        #if not self.train: # fix seed for validation and test\n",
    "        #    np.random.seed(3)\n",
    "        sample = self.df.groupby(\"subject_id\", group_keys=False).apply(lambda x: x.sample(k, replace=True))\n",
    "        sample = sample.copy()\n",
    "        if self.train:\n",
    "            self.subject_index = [self.id2index[subject_id] for subject_id in sample.subject_id.values]\n",
    "            self.random = np.random.choice(2, sample.shape[0], p=[0.1, 0.9])\n",
    "            self.subject_index = self.subject_index*self.random\n",
    "        self.df_sample = sample\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df_sample.iloc[index,:]\n",
    "        x_series = (row.series - self.norm_dict[\"series\"][0])/self.norm_dict[\"series\"][1]\n",
    "        x_cont = [(row[name]-self.norm_dict[name][0])/self.norm_dict[name][1] for name in self.names]\n",
    "        x_binary = [row[name] for name in self.names_binary]\n",
    "        subject_index = 0\n",
    "        if self.train:\n",
    "            subject_index = self.subject_index[index]\n",
    "        x_cat = np.array([row[\"care_unit\"], subject_index])\n",
    "        x_cont = np.array(x_cont + x_binary)\n",
    "        return x_series, x_cont, x_cat, row[\"prediction_mean_HR\"], row[\"prediction_mean_MAP\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventModel3(nn.Module):\n",
    "    def __init__(self, num_subjects, hidden_size=100, num2=50):\n",
    "        super(EventModel3, self).__init__()\n",
    "        self.embedding1 = nn.Embedding(5, 1)\n",
    "        self.embedding2 = nn.Embedding(num_subjects+1, 5)\n",
    "        \n",
    "        self.gru = nn.GRU(5, hidden_size, num_layers=2, batch_first=True,\n",
    "                          dropout=0.3)\n",
    "        self.num1 = hidden_size + 1 + 5 + 7\n",
    "        self.num2 = num2\n",
    "        self.linear1 = nn.Linear(self.num1, self.num2)\n",
    "        self.linear2 = nn.Linear(self.num2, self.num2)\n",
    "        self.out1 = nn.Linear(self.num2, 1)\n",
    "        self.out2 = nn.Linear(self.num2, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(self.num2)\n",
    "        self.bn2 = nn.BatchNorm1d(self.num2)\n",
    "        \n",
    "    def forward(self, x_series, x_cont, x_cat):\n",
    "        _, ht = self.gru(x_series)\n",
    "        x_cat_1 = self.embedding1(x_cat[:,0])\n",
    "        x_cat_2 = self.embedding2(x_cat[:,1])\n",
    "        x = torch.cat((ht[-1], x_cat_1, x_cat_2, x_cont), 1)\n",
    "        x = self.bn1(F.relu(self.linear1(x)))\n",
    "        x = self.bn2(F.relu(self.linear2(x)))\n",
    "        return self.out1(x), self.out2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventModel3_single(nn.Module):\n",
    "    def __init__(self, num_subjects, hidden_size=100, num2=50):\n",
    "        super(EventModel3_single, self).__init__()\n",
    "        self.embedding1 = nn.Embedding(5, 1)\n",
    "        self.embedding2 = nn.Embedding(num_subjects+1, 5)\n",
    "        \n",
    "        self.gru = nn.GRU(5, hidden_size, num_layers=2, batch_first=True,\n",
    "                          dropout=0.3)\n",
    "        self.num1 = hidden_size + 1 + 5 + 7\n",
    "        self.num2 = num2\n",
    "        self.linear1 = nn.Linear(self.num1, self.num2)\n",
    "        self.linear2 = nn.Linear(self.num2, self.num2)\n",
    "        self.out = nn.Linear(self.num2, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(self.num2)\n",
    "        self.bn2 = nn.BatchNorm1d(self.num2)\n",
    "        \n",
    "    def forward(self, x_series, x_cont, x_cat):\n",
    "        _, ht = self.gru(x_series)\n",
    "        x_cat_1 = self.embedding1(x_cat[:,0])\n",
    "        x_cat_2 = self.embedding2(x_cat[:,1])\n",
    "        x = torch.cat((ht[-1], x_cat_1, x_cat_2, x_cont), 1)\n",
    "        x = self.bn1(F.relu(self.linear1(x)))\n",
    "        x = self.bn2(F.relu(self.linear2(x)))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventModel33(nn.Module):\n",
    "    def __init__(self, num_subjects, hidden_size=100, num2=50, single=False):\n",
    "        super(EventModel33, self).__init__()\n",
    "        self.single = single\n",
    "        self.embedding1 = nn.Embedding(5, 1)\n",
    "        self.embedding2 = nn.Embedding(num_subjects+1, 5)\n",
    "\n",
    "        self.gru = nn.GRU(5, hidden_size, num_layers=2, batch_first=True,\n",
    "                          dropout=0.3)\n",
    "        self.num1 = hidden_size + 1 + 5 + 7\n",
    "        self.num2 = num2\n",
    "        self.linear1 = nn.Linear(self.num1, self.num2)\n",
    "        self.linear2 = nn.Linear(self.num2, self.num2)\n",
    "        self.out1 = nn.Linear(self.num2, 1)\n",
    "        self.out2 = nn.Linear(self.num2, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(self.num2)\n",
    "        self.bn2 = nn.BatchNorm1d(self.num2)\n",
    "\n",
    "    def forward(self, x_series, x_cont, x_cat):\n",
    "        _, ht = self.gru(x_series)\n",
    "        x_cat_1 = self.embedding1(x_cat[:,0])\n",
    "        x_cat_2 = self.embedding2(x_cat[:,1])\n",
    "        x = torch.cat((ht[-1], x_cat_1, x_cat_2, x_cont), 1)\n",
    "        x = self.bn1(F.relu(self.linear1(x)))\n",
    "        x = self.bn2(F.relu(self.linear2(x)))\n",
    "        if self.single:\n",
    "            return self.out1(x)\n",
    "        else:\n",
    "            return self.out1(x), self.out2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics(model, modelS, test_ds):\n",
    "    model.eval()\n",
    "    modelS.eval()\n",
    "    test_ds.pick_a_sample()\n",
    "    test_dl = DataLoader(test_ds, batch_size=len(test_ds))\n",
    "    for x_series, x_cont, x_cat, y1, y2 in test_dl:\n",
    "        batch = y1.shape[0]\n",
    "        x_series = x_series.float().cuda()\n",
    "        x_cont = x_cont.float().cuda()\n",
    "        x_cat = x_cat.long().cuda()\n",
    "        y1 = y1.float().cuda()\n",
    "        out1, out2 = model(x_series, x_cont, x_cat)\n",
    "        out = modelS(x_series, x_cont, x_cat)\n",
    "       \n",
    "    y_hat = out2.view(-1).detach().cpu().numpy()\n",
    "    y_hatS = out.view(-1).detach().cpu().numpy()\n",
    "    ys2 = y2.view(-1).cpu().numpy()\n",
    "    \n",
    "    r2 = metrics.r2_score(ys2, y_hat)\n",
    "    r2S = metrics.r2_score(ys2, y_hatS)\n",
    "    return r2, r2S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boostrap_test_metrics(model, modelS, test_ds, N=1000):\n",
    "    r2s = []\n",
    "    r2Ss = []\n",
    "    for i in range(N):\n",
    "        r2, r2S = test_metrics(model, modelS, test_ds)\n",
    "        r2s.append(r2)\n",
    "        r2Ss.append(r2S)\n",
    "    return r2s, r2Ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gap = \"60min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = get_data_gap(gap=\"60min\")\n",
    "norm_dict, care2id, id2index, num_subjects = stats_dict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_validation_60min.pickle data_test_60min.pickle\n"
     ]
    }
   ],
   "source": [
    "test_ext, test = get_test_data_gap(gap=\"60min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1461, 17), 2184, 559)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = MultiTask(test, norm_dict, id2index, care2id, k=13, train=False)\n",
    "test_ext_ds = MultiTask(test_ext, norm_dict, id2index, care2id, k=13, train=False)\n",
    "test.shape, len(test_ds), len(test_ext_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = PATH/\"models/model3_60min_r2_58_71.pth\"\n",
    "model = EventModel3(num_subjects).cuda()\n",
    "load_model(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = PATH/\"models/model3_single_60min_r2_70.pth\"\n",
    "modelS = EventModel3_single(num_subjects).cuda()\n",
    "load_model(modelS, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=-27.21127432542325, pvalue=9.458268957079656e-48)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2, r2S = boostrap_test_metrics(model, modelS, test_ds, N=100)\n",
    "stats.ttest_rel(r2, r2S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01420232, -0.0079482 , -0.0017322 ])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(np.array(r2) - np.array(r2S), [0.025, 0.5, 0.975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=51.68264094650364, pvalue=1.9553414107002766e-73)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2, r2S = boostrap_test_metrics(model, modelS, test_ext_ds, N=100)\n",
    "stats.ttest_rel(r2, r2S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01958162, 0.03161031, 0.04319205])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(np.array(r2) - np.array(r2S), [0.025, 0.5, 0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gap = \"30min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_validation_30min.pickle data_test_30min.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2733, 17), 2379, 637)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap=\"30min\"\n",
    "train, valid = get_data_gap(gap=gap)\n",
    "norm_dict, care2id, id2index, num_subjects = stats_dict(train)\n",
    "test_ext, test = get_test_data_gap(gap=gap)\n",
    "\n",
    "test_ds = MultiTask(test, norm_dict, id2index, care2id, k=13, train=False)\n",
    "test_ext_ds = MultiTask(test_ext, norm_dict, id2index, care2id, k=13, train=False)\n",
    "test.shape, len(test_ds), len(test_ext_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = PATH/\"models/model3_30min_r2_87_78.pth\"\n",
    "pathS = PATH/\"models/model3_single_30min_r2_78.pth\"\n",
    "model = EventModel3(num_subjects).cuda()\n",
    "load_model(model, path)\n",
    "modelS = EventModel3_single(num_subjects).cuda()\n",
    "load_model(modelS, pathS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=-0.11315492642648424, pvalue=0.9101368707014197)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2, r2S = boostrap_test_metrics(model, modelS, test_ds, N=100)\n",
    "stats.ttest_rel(r2, r2S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00443137, -0.00021609,  0.00461935])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(np.array(r2) - np.array(r2S), [0.025, 0.5, 0.975])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=83.48001787893983, pvalue=1.412715132017392e-93)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2, r2S = boostrap_test_metrics(model, modelS, test_ext_ds, N=100)\n",
    "stats.ttest_rel(r2, r2S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04302278, 0.05549248, 0.06590367])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(np.array(r2) - np.array(r2S), [0.025, 0.5, 0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gap = \"15min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_validation_15min.pickle data_test_15min.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4442, 17), 2587, 637)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap=\"15min\"\n",
    "train, valid = get_data_gap(PATH, gap=gap)\n",
    "norm_dict, care2id, id2index, num_subjects = stats_dict(train)\n",
    "test_ext, test = get_test_data_gap(PATH, gap=gap)\n",
    "\n",
    "test_ds = MultiTask(test, norm_dict, id2index, care2id, k=13, train=False)\n",
    "test_ext_ds = MultiTask(test_ext, norm_dict, id2index, care2id, k=13, train=False)\n",
    "test.shape, len(test_ds), len(test_ext_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EventModel33:\n\tMissing key(s) in state_dict: \"out2.weight\", \"out2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b2b3a6e44d72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpathS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"models/modelX3_single_15min_r2_85.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEventModel33\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_subjects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodelS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEventModel33\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_subjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-4fce61fa246c>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(m, p)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 830\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EventModel33:\n\tMissing key(s) in state_dict: \"out2.weight\", \"out2.bias\". "
     ]
    }
   ],
   "source": [
    "path = PATH/\"models/modelX3_15min_r2_91_85.pth\"\n",
    "pathS = path = PATH/\"models/modelX3_single_15min_r2_85.pth\"\n",
    "model = EventModel33(num_subjects).cuda()\n",
    "load_model(model, path)\n",
    "modelS = EventModel33(num_subjects, single=True).cuda()\n",
    "load_model(modelS, pathS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
